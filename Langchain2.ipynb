{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dbf547-c695-40e6-8c25-a5af8235d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942e3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e739de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32312d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41840c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef57e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed942f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Loaded:\", \"Yes\" if hf_token else \"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc8c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4547599",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_model = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\": 0.3, \"max_length\": 500},\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b111fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c2b3ca70e74938a632c3fd6f28f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Microsoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2489e8333a0a4b35b9a43433ce04db7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/63526f4c7e4cc3135fd0ff1a/6acb653f3c05b8398d386c9e96027f0bde0a3e41095eb0278514608891324f49?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250516%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250516T041836Z&X-Amz-Expires=3600&X-Amz-Signature=67c164a5e04ef35a85dda073392215c18bb4b9dc4344707b105854c5360ca853&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1747372716&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzM3MjcxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MzUyNmY0YzdlNGNjMzEzNWZkMGZmMWEvNmFjYjY1M2YzYzA1YjgzOThkMzg2YzllOTYwMjdmMGJkZTBhM2U0MTA5NWViMDI3ODUxNDYwODg5MTMyNGY0OSoifV19&Signature=QpuoIn3sDS33IlHYik7t6ecwSHen9uDVL62JKra%7EPbUrmlR43TT0AnEjC-sNDHV9CNN29lHGYJvDBYa26EodDxcJMjIToYfEzf82EGk76beb25NpCBZRh8ukS6MoXiFASRHj46AQ1kdayPmFHdZO8xOmv4-w21ivoyI47Yv6x7tkweijoUYyadgj62bQoVA0S5AEqE6rAiq8CcTiWRkg02rXl4Ky1bPkpxeTgNQsgXEPUvtKLANZkHnT8coqQK5zgrHa7MqLo8uBVtEYmidGU2b5lwNOVeSN8oRTG4VX37hsNvCqFchP-ny18QDpF9csS-1h7FiWXgj8%7EKVXWHSJhg__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e5a8a4174441e799417ca0ba5d2bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  92%|#########2| 2.89G/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/63526f4c7e4cc3135fd0ff1a/6acb653f3c05b8398d386c9e96027f0bde0a3e41095eb0278514608891324f49?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250516%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250516T044908Z&X-Amz-Expires=3600&X-Amz-Signature=fc5aa5d2698cfa2bd2e044dc40d891b5f3caabc47b610458d2ecfbd91f95d3cb&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1747374548&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzM3NDU0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MzUyNmY0YzdlNGNjMzEzNWZkMGZmMWEvNmFjYjY1M2YzYzA1YjgzOThkMzg2YzllOTYwMjdmMGJkZTBhM2U0MTA5NWViMDI3ODUxNDYwODg5MTMyNGY0OSoifV19&Signature=OatYOVlzXfeOHa4A42pHh0qLstQB6wcoTjSMfMho9mjdFIvBZXDQ5AGwBl7l2Lld8mDmnKhpSF8wfFIswAzuYmDNChb7NZxnsHb9-ntouLSG-GfRR-e5KbzcyzHQa6x74u16p8TvgyCucmwcIspMNTF5gS2oQVwuYW-f5Xc3wYRUhZ499XYIFlFCRhDrnAPeXoqKHZ2wBsqb-MZ0%7E9dmYqLkx-6eoCHbFtaEWB87XfbScH1qX30Gsc%7EiDKZkQ5GJ2eoNc9RrYS2VpMWGIOUz0%7EX6QBZz1kjKj2O2HUC4XzZxnX5Fz2c8zgRzM0%7EPtnYHqK3O4X4HH8iO94XaYlK2eQ__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6c9ed2c3f5405ab186d471c270e29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  99%|#########9| 3.10G/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Microsoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Microsoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada103db80124d32bb5bb374da443455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357bdb6e77f54dc9a24ac41c7892c6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c5f85a816e497ebbe1271d878c9739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9246bb399f4e11824fb3dffb02a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n",
      "c:\\Users\\Microsoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:836: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a programming language , a computer programming language , and a computer\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "result = pipe(\"Please explain what Python is, including its purpose, features, and why it is popular.\")\n",
    "print(result[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86719840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a73684",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf02e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
      "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
      "Various subfields of AI research are centered around particular goals and the use of particular tools.\n"
     ]
    }
   ],
   "source": [
    "topic = \"artificial intelligence\"\n",
    "summary = wikipedia.summary(topic, sentences=5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8487ea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an expert teacher\n",
      "    simplify the following information for a 10 year old student:\n",
      "    Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
      "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
      "Various subfields of AI research are centered around particular goals and the use of particular tools.\n",
      "    answer in simple and short sentences\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = PromptTemplate(\n",
    "    input_variables= [\"text\"],\n",
    "    template=\"\"\"you are an expert teacher\n",
    "    simplify the following information for a 10 year old student:\n",
    "    {text}\n",
    "    answer in simple and short sentences\"\"\"\n",
    "      \n",
    ")\n",
    "prompt = template.format(text=summary)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "421e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffe3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae3b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3285383344.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  groq_chat = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level programming language that is easy to learn and understand, even for people who don't have a background in computer science. Here's a simple explanation:\n",
      "\n",
      "**What is Python?**\n",
      "\n",
      "Python is a way to communicate with computers using a special set of instructions called \"code\". Just like how you write letters to communicate with people, Python allows you to write code to communicate with computers.\n",
      "\n",
      "**What can you do with Python?**\n",
      "\n",
      "With Python, you can:\n",
      "\n",
      "1. **Create programs**: Write code to automate tasks, solve problems, or create games.\n",
      "2. **Analyze data**: Use Python to collect, clean, and analyze data from various sources.\n",
      "3. **Build websites**: Create dynamic websites using Python and popular frameworks like Django or Flask.\n",
      "4. **Automate tasks**: Use Python to automate repetitive tasks, like sending emails or updating spreadsheets.\n",
      "5. **Create artificial intelligence**: Use Python to build AI-powered applications, like chatbots or image recognition systems.\n",
      "\n",
      "**Why is Python popular?**\n",
      "\n",
      "Python is popular because:\n",
      "\n",
      "1. **Easy to learn**: Python has a simple syntax and is relatively easy to learn, even for beginners.\n",
      "2. **Fast development**: Python allows you to build programs quickly, making it a great choice for rapid prototyping.\n",
      "3. **Large community**: Python has a massive community of developers who create and share libraries, frameworks, and resources.\n",
      "4. **Cross-platform**: Python can run on multiple operating systems, including Windows, macOS, and Linux.\n",
      "\n",
      "**In simple words...**\n",
      "\n",
      "Python is a programming language that lets you communicate with computers using a special set of instructions. With Python, you can create programs, analyze data, build websites, automate tasks, and even create artificial intelligence. It's a popular language because it's easy to learn, fast to develop, and has a large community of developers who create and share resources.\n"
     ]
    }
   ],
   "source": [
    "groq_chat = ChatOpenAI(\n",
    "    model_name = \"llama3-8b-8192\",\n",
    "    base_url = \"https://api.groq.com/openai/v1\",\n",
    "    temperature = 0.5,\n",
    "    max_tokens =500\n",
    ")\n",
    "\n",
    "response = groq_chat.invoke(\"explain what is python programming language in simple words\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "701673d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31773222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exlpain machine learning in simple words\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=\"exlpain {concept} in simple words\"\n",
    ")\n",
    "final_prompt = template.format(concept=\"machine learning\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eee3b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an expert teacher\n",
      "    Explain the concept of python programming to a beginner audience.\n",
      "    use simple language if level is beginner else use technical depth.\n",
      "you are an expert teacher\n",
      "    Explain the concept of python programming to a expert audience.\n",
      "    use simple language if level is beginner else use technical depth.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"concept\", \"level\"],\n",
    "    template=\"\"\"you are an expert teacher\n",
    "    Explain the concept of {concept} to a {level} audience.\n",
    "    use simple language if level is beginner else use technical depth.\"\"\"\n",
    ")\n",
    "prompt1 = template.format(concept=\"python programming\", level=\"beginner\")\n",
    "prompt2 = template.format(concept=\"python programming\", level=\"expert\")\n",
    "print(prompt1)\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba0f99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a delight! I'm thrilled to share my expertise on Python programming with an expert audience. I'll strike a balance between simplicity and technical depth, catering to both beginners and advanced learners.\n",
      "\n",
      "**What is Python Programming?**\n",
      "\n",
      "Python is a high-level, interpreted programming language that emphasizes code readability and simplicity. It's a general-purpose language, meaning it can be used for various applications, such as web development, scientific computing, data analysis, artificial intelligence, and more.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Interpreted Language**: Python code is executed line by line, without the need for compilation. This makes it an ideal choice for rapid prototyping, testing, and development.\n",
      "2. **High-Level Language**: Python abstracts away many low-level details, allowing developers to focus on the logic of their program rather than memory management and other implementation details.\n",
      "3. **Object-Oriented Programming (OOP)**: Python supports OOP concepts like classes, objects, inheritance, polymorphism, and encapsulation, making it easy to organize and structure code.\n",
      "4. **Dynamic Typing**: Python is dynamically typed, meaning you don't need to declare variable types before using them. This flexibility makes it easier to write code and reduces the likelihood of errors.\n",
      "5. **Large Standard Library**: Python's extensive standard library includes modules for various tasks, such as file I/O, networking, and data structures, making it easy to find the right tool for the job.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "1. **Easy to Learn**: Python has a simple syntax and is relatively easy to pick up, even for those without prior programming experience.\n",
      "2. **Fast Development**: Python's interpreted nature and extensive libraries enable rapid development and prototyping.\n",
      "3. **Cross-Platform**: Python code can run on multiple platforms, including Windows, macOS, and Linux, without modification.\n",
      "4. **Large Community**: Python has a vast and active community, ensuring there are many resources available for learning and troubleshooting.\n",
      "\n",
      "**Common Use Cases:**\n",
      "\n",
      "1. **Web Development**: Python is used in web development frameworks like Django, Flask, and Pyramid, making it a popular choice for building web applications.\n",
      "2. **Data Science and Machine Learning**: Python is widely used in data science and machine learning due to its simplicity, flexibility, and extensive libraries like NumPy, pandas, and scikit-learn.\n",
      "3. **Automation**: Python is often used for automating tasks, such as data processing, file management, and\n"
     ]
    }
   ],
   "source": [
    "resp = groq_chat.invoke(prompt2)\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d1247a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! I'm excited to introduce you to the fascinating world of Deep Learning!\n",
      "\n",
      "Imagine you're trying to teach a child to recognize different animals. You show them a picture of a cat, and they say, \"Oh, that's a cat!\" But then you show them a picture of a dog, and they say, \"That's also a cat!\" Oops, not quite!\n",
      "\n",
      "This is where Deep Learning comes in. It's a type of Artificial Intelligence (AI) that helps machines learn and recognize patterns, just like humans do. But instead of getting confused like our child friend, Deep Learning models can learn to recognize patterns and make accurate predictions.\n",
      "\n",
      "Here's a simple analogy to help you understand:\n",
      "\n",
      "**Deep Learning is like building a hierarchical house of cards**\n",
      "\n",
      "Imagine you have a deck of cards with different animals on them (e.g., cat, dog, bird, etc.). You want to build a house of cards that can recognize these animals. Here's how you'd do it:\n",
      "\n",
      "1. **Base layer**: You start by building a simple layer of cards that recognize basic features, like \"is it a mammal?\" or \"does it have four legs?\"\n",
      "2. **Middle layer**: Next, you add a layer of cards that build upon the base layer. These cards recognize more complex features, like \"is it a carnivore?\" or \"does it have a tail?\"\n",
      "3. **Top layer**: Finally, you add a layer of cards that make predictions based on the features\n"
     ]
    }
   ],
   "source": [
    "groq = ChatOpenAI(\n",
    "    model_name = \"llama3-8b-8192\",\n",
    "    base_url = \"https://api.groq.com/openai/v1\",\n",
    "    temperature = 0.5,\n",
    "    max_tokens = 300\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"concept\", \"audience\"],\n",
    "    template= \"\"\"You are an expert teacher.\n",
    "Explain the concept of {concept} to a {audience} audience.\n",
    "Use examples and simple language.\"\"\"\n",
    ")\n",
    "final_prompt = prompt.format(\n",
    "    concept = \"Deep learning\",\n",
    "    audience = \"beginner\"\n",
    ")\n",
    "res = groq.invoke(final_prompt)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b6263fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three benefits of Artificial Intelligence (AI):\n",
      "\n",
      "1. **Improved Efficiency**: AI can automate repetitive and mundane tasks, freeing up human resources to focus on more strategic and creative work. For example, AI-powered chatbots can handle customer inquiries, reducing the need for human customer support agents to handle routine requests. This can lead to increased productivity and reduced costs.\n",
      "2. **Enhanced Decision-Making**: AI can analyze large amounts of data quickly and accurately, providing insights that can inform better decision-making. For instance, AI-powered predictive analytics can help businesses forecast demand, identify trends, and optimize supply chain management. This can lead to more informed business decisions and improved outcomes.\n",
      "3. **Improved Healthcare Outcomes**: AI can help improve healthcare outcomes by analyzing medical images, detecting diseases earlier, and personalizing treatment plans. For example, AI-powered computer vision can help radiologists detect breast cancer more accurately, while AI-powered chatbots can help patients manage chronic diseases and adhere to treatment plans. This can lead to better patient outcomes, reduced healthcare costs, and improved quality of life.\n",
      "\n",
      "These are just a few examples of the many benefits of AI. As the technology continues to evolve, we can expect to see even more innovative applications across various industries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "pro = \"list three benefits of Artificial intelligence.\"\n",
    "res1 = groq.invoke(pro)\n",
    "parser = StrOutputParser()\n",
    "clean_output = parser.invoke(res1)\n",
    "print(clean_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef26723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Artificial Intelligence (AI) refers to the broader field of research and development aimed at creating machines that can perform tasks that typically require human intelligence, such as understanding language, recognizing images, making decisions, and solving problems. Machine Learning (ML) is a subset of AI that involves training machines to learn from data, improve their performance on a task over time, and make predictions or decisions based on that data. In other words, AI is the umbrella term that encompasses ML, as well as other approaches to creating intelligent machines, such as rule-based systems and expert systems. While all ML is AI, not all AI is ML. AI can be rule-based, using pre-defined rules to make decisions, whereas ML is based on patterns learned from data.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Create the prompt template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Please answer the following question and output your answer only in JSON format.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Format:\n",
    "    {{\n",
    "      \"answer\": \"your detailed answer here\"\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "formatted_prompt = prompt.format(question=\"What is the difference between AI and ML?\")\n",
    "\n",
    "response = groq.invoke(formatted_prompt)\n",
    "\n",
    "parsed_output = parser.invoke(response)\n",
    "\n",
    "print(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03755df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'AI in Education: A Brighter Tomorrow', 'summary': 'Artificial Intelligence is revolutionizing the education sector by providing personalized learning experiences, automating administrative tasks, and enhancing teacher-student interactions. AI-powered adaptive learning systems can identify knowledge gaps and adjust curricula accordingly, while natural language processing (NLP) enables more effective communication between teachers and students. As AI continues to evolve, it has the potential to democratize access to quality education and bridge the gap between developed and developing countries.', 'tags': ['Artificial Intelligence', 'Education', 'Personalized Learning', 'Adaptive Learning', 'Natural Language Processing'], 'difficulty': 'medium'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Create the output parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Create complex prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a professional json api.\n",
    "    Answer ONLY with a valid JSON. NO extra text. NO explanations.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Format:\n",
    "    {{\n",
    "      \"title\": \"Short creative title\",\n",
    "      \"summary\": \"Detailed explanation in 3-5 sentences\",\n",
    "      \"tags\": [\"relevant\", \"keywords\", \"separated\", \"as\", \"array\"],\n",
    "      \"difficulty\": \"easy/medium/hard\"\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Format the question\n",
    "formatted_prompt = prompt.format(\n",
    "    question=\"What is the future of Artificial Intelligence in education?\"\n",
    ")\n",
    "\n",
    "# Send prompt to model\n",
    "response3 = groq.invoke(formatted_prompt)\n",
    "\n",
    "# Parse the response\n",
    "parsed_output1 = parser.invoke(response3)\n",
    "\n",
    "# Final output\n",
    "print(parsed_output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ce838fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\4161266607.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Tate is a British-American social media personality, professional kickboxer, and entrepreneur. He was born on December 14, 1976, in Washington, D.C., USA, to a Romanian mother and a British father. He later moved to Luton, England, where he grew up.\n",
      "\n",
      "Tate gained fame in the early 2000s after winning the World Kickboxing Association (WKA) world championship in 2005. He also competed in the reality TV show \"Big Brother\" in 2016, where he finished as the runner-up.\n",
      "\n",
      "In recent years, Tate has become popular on social media platforms such as Twitter, TikTok, and YouTube, where he shares his opinions on various topics, including politics, culture, and social issues. He has gained a large following, particularly among young men, and has been described as a \"social media influencer\" or a \"thought leader.\"\n",
      "\n",
      "However, Tate has also been criticized for his controversial views, which have been perceived as misogynistic, homophobic, and xenophobic. He has been accused of making sexist and racist remarks, and has been involved in several public feuds with other celebrities and public figures.\n",
      "\n",
      "In addition to his online presence, Tate has also been involved in various business ventures, including real estate investing, cryptocurrency trading, and online marketing. He has also written a book titled \"The New World Order\" and has been involved in various philanthropic efforts, including supporting charities that help homeless people.\n",
      "\n",
      "Overall\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"who is {person}?\")\n",
    "chain = LLMChain(\n",
    "    llm=groq,\n",
    "    prompt=prompt\n",
    "    )\n",
    "response = chain.invoke({\"person\":\"Andrew Tate\"})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23a6f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': 'Sam Altman', 'text': \"Sam Altman is an American entrepreneur, programmer, and investor. He was born on April 22, 1985, in Chicago, Illinois. Altman is best known for being the CEO of Y Combinator, a well-known startup accelerator and seed fund, and for his involvement in various other tech companies and projects.\\n\\nEarly Life and Education:\\n\\nAltman grew up in Chicago and developed an interest in computer programming at a young age. He attended Phillips Exeter Academy and later studied computer science at Stanford University, where he graduated in 2007.\\n\\nCareer:\\n\\nAfter graduating from Stanford, Altman worked at several startups, including a company called Loopt, which was acquired by Green Dot Corporation in 2012. In 2009, he co-founded a company called Lift, which was a social networking site that allowed users to share their goals and accomplishments with friends.\\n\\nIn 2011, Altman joined Y Combinator as a partner, and in 2014, he became the company's CEO. Under his leadership, Y Combinator has invested in and supported many successful startups, including Airbnb, Dropbox, and Reddit.\\n\\nIn addition to his work at Y Combinator, Altman has also been involved in various other projects and companies. In 2013, he co-founded a company called Viaweb, which was a web-based spreadsheet application. He has also been a vocal advocate for the use of artificial intelligence and blockchain technology in various industries.\\n\\nPersonal Life\", 'summary': 'Here is a summary of the text in 1 sentence:\\n\\nSam Altman is a successful American entrepreneur, programmer, and investor who has held various leadership roles, including CEO of Y Combinator, and has been involved in numerous startups and projects, including Lift, Viaweb, and various investments in successful companies like Airbnb, Dropbox, and Reddit.', 'hashtags': 'Here are three possible hashtags for the summary:\\n\\n1. #Entrepreneurship\\n2. #TechLeaders\\n3. #StartupSuccess'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain , SequentialChain\n",
    "\n",
    "prom1 = PromptTemplate.from_template(\"who is {person} in detail.\")\n",
    "prom2 = PromptTemplate.from_template(\"summarize this text in 1 sentence:\\n {text}\")\n",
    "prom3 = PromptTemplate.from_template(\"generate 3 hashtags for this summary:\\n{summary}\")\n",
    "\n",
    "chain1= LLMChain(llm=groq, prompt=prom1, output_key=\"text\")\n",
    "chain2= LLMChain(llm=groq, prompt=prom2, output_key=\"summary\")\n",
    "chain3=LLMChain(llm=groq, prompt=prom3, output_key=\"hashtags\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3],\n",
    "    input_variables=[\"person\"],\n",
    "    output_variables=[\"text\", \"summary\", \"hashtags\"],\n",
    ")\n",
    "response = overall_chain.invoke({\"person\":\"Sam Altman\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdda4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the theory of relativity?', 'text': 'The theory of relativity, proposed by Albert Einstein in 1905 and 1915, is a fundamental concept in modern physics that revolutionized our understanding of space, time, and gravity. The theory consists of two main components: special relativity and general relativity.\\n\\n**Special Relativity (1905)**\\n\\nSpecial relativity posits that the laws of physics are the same for all observers in uniform motion relative to one another. This theory challenged the long-held notion of absolute time and space, which was based on the concept of a fixed, unchanging background against which objects move. Instead, Einstein introduced the concept of spacetime, which is a four-dimensional fabric that combines space and time.\\n\\nThe theory of special relativity is based on two postulates:\\n\\n1. The laws of physics are the same for all observers in uniform motion relative to one another.\\n2. The speed of light is always constant, regardless of the motion of the observer or the source of light.\\n\\nFrom these postulates, Einstein derived several important consequences, including:\\n\\n* Time dilation: Time appears to pass slower for an observer in motion relative to a stationary observer.\\n* Length contraction: Objects appear shorter to an observer in motion relative to a stationary observer.\\n* Relativity of simultaneity: Two events that are simultaneous for one observer may not be simultaneous for another observer in a different state of motion.\\n* Equivalence of mass and energy: Mass and energy are interchangeable, as expressed by the famous equation E ='}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Assuming you already have groq LLM\n",
    "# llm = ChatGroq(....)\n",
    "\n",
    "# 1. Define prompts\n",
    "science_prompt = PromptTemplate.from_template(\n",
    "    \"Answer this science question in detail:\\nQuestion: {question}\"\n",
    ")\n",
    "history_prompt = PromptTemplate.from_template(\n",
    "    \"Answer this historical event:\\nQuestion: {question}\"\n",
    ")\n",
    "default_prompt = PromptTemplate.from_template(\n",
    "    \"Answer this casually:\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# 2. Create chains\n",
    "science_chain = LLMChain(llm=groq, prompt=science_prompt)\n",
    "history_chain = LLMChain(llm=groq, prompt=history_prompt)\n",
    "default_chain = LLMChain(llm=groq, prompt=default_prompt)\n",
    "\n",
    "destination_chains = {\n",
    "    \"science\": science_chain,\n",
    "    \"history\": history_chain,\n",
    "    \"default\": default_chain,\n",
    "}\n",
    "\n",
    "# 3. Create a simple router function\n",
    "def simple_router(inputs):\n",
    "    question = inputs[\"question\"].lower()\n",
    "    if any(word in question for word in [\"relativity\", \"physics\", \"atom\", \"experiment\", \"science\"]):\n",
    "        return {\"chain_name\": \"science\", \"question\": inputs[\"question\"]}\n",
    "    elif any(word in question for word in [\"war\", \"revolution\", \"empire\", \"ancient\", \"history\"]):\n",
    "        return {\"chain_name\": \"history\", \"question\": inputs[\"question\"]}\n",
    "    else:\n",
    "        return {\"chain_name\": \"default\", \"question\": inputs[\"question\"]}\n",
    "\n",
    "# 4. Wrap router into Runnable\n",
    "router = RunnableLambda(simple_router)\n",
    "\n",
    "# 5. Now combine everything manually\n",
    "def route_and_invoke(inputs):\n",
    "    route_result = router.invoke(inputs)\n",
    "    chain_name = route_result[\"chain_name\"]\n",
    "    question = route_result[\"question\"]\n",
    "    selected_chain = destination_chains[chain_name]\n",
    "    return selected_chain.invoke({\"question\": question})\n",
    "\n",
    "# 6. Wrap final router into a RunnableLambda\n",
    "final_chain = RunnableLambda(route_and_invoke)\n",
    "\n",
    "# 7. Now test it\n",
    "question = \"What is the theory of relativity?\"\n",
    "output = final_chain.invoke({\"question\": question})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df1bc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\2138785304.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is mark zuckerberg?', 'chat_history': '', 'text': 'Mark Zuckerberg is an American technology entrepreneur and philanthropist. He is best known as the co-founder and CEO of the social media platform Facebook, which he launched in 2004 with his college roommates and fellow Harvard University students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes.'}\n",
      "{'question': 'What is he famous for?', 'chat_history': 'Human: Who is mark zuckerberg?\\nAI: Mark Zuckerberg is an American technology entrepreneur and philanthropist. He is best known as the co-founder and CEO of the social media platform Facebook, which he launched in 2004 with his college roommates and fellow Harvard University students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes.', 'text': 'Mark Zuckerberg is famous for co-founding and leading the social media platform Facebook, which has become one of the most widely used and influential social media platforms in the world. He is also known for his philanthropic efforts, particularly through the Chan Zuckerberg Initiative, which he founded with his wife Priscilla Chan.'}\n",
      "{'question': 'Tell me one of his theories.', 'chat_history': 'Human: Who is mark zuckerberg?\\nAI: Mark Zuckerberg is an American technology entrepreneur and philanthropist. He is best known as the co-founder and CEO of the social media platform Facebook, which he launched in 2004 with his college roommates and fellow Harvard University students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes.\\nHuman: What is he famous for?\\nAI: Mark Zuckerberg is famous for co-founding and leading the social media platform Facebook, which has become one of the most widely used and influential social media platforms in the world. He is also known for his philanthropic efforts, particularly through the Chan Zuckerberg Initiative, which he founded with his wife Priscilla Chan.', 'text': 'Mark Zuckerberg has advocated for the concept of \"The Immortal Lifeboat\". This theory proposes that humanity should prioritize developing technologies that would allow us to preserve human consciousness and memories, even if our physical bodies were to become extinct. He believes that this could be achieved through brain-computer interfaces, artificial intelligence, and other advanced technologies.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\",\"question\"],\n",
    "    template=\"\"\"You are a helpful chatbot.\n",
    "\n",
    "Here is the chat history:\n",
    "{chat_history}\n",
    "\n",
    "User: {question}\n",
    "Chatbot:\"\"\"\n",
    ")\n",
    "chat_chain = LLMChain(\n",
    "    llm=groq,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "print(chat_chain.invoke({\"question\": \"Who is mark zuckerberg?\"}))\n",
    "print(chat_chain.invoke({\"question\": \"What is he famous for?\"}))\n",
    "print(chat_chain.invoke({\"question\": \"Tell me one of his theories.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7a2f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3145384436.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=groq)\n",
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3145384436.py:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is John.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary:\n",
      "None (new conversation)\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Hi, my name is John.\n",
      "AI: Nice to meet you, John! I'm Ada, your friendly AI companion. I've been designed to assist and converse with humans, and I'm excited to get to know you better. By the way, I've been trained on a vast amount of text data, including books, articles, and conversations, which enables me to provide accurate and informative responses. I'm also constantly learning and updating my knowledge base to stay current with the latest developments in various fields. What would you like to talk about, John?\n",
      "\n",
      "New summary:\n",
      "John introduces himself to Ada, and Ada introduces herself as a friendly AI companion designed to assist and converse with humans.\n",
      "Human: I love playing football.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary:\n",
      "John introduces himself to Ada, and Ada introduces herself as a friendly AI companion designed to assist and converse with humans.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: I love playing football.\n",
      "AI: Football! A popular sport indeed! I've been trained on a vast amount of text data, including sports articles and news, which allows me to provide you with some interesting facts about football. Did you know that the first official football match was played in 1863 in England, and the rules were established by the Football Association? The sport has since become a global phenomenon, with millions of fans worldwide. Which team do you support, John?\n",
      "\n",
      "New summary:\n",
      "John introduces himself to Ada, and Ada introduces herself as a friendly AI companion designed to assist and converse with humans. John expresses his love for playing football, and Ada shares some interesting facts about the sport, including its origins and global popularity.\n",
      "Human: Do you remember what my favorite sport is?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you remember what my favorite sport is?',\n",
       " 'history': \"Current summary:\\nJohn introduces himself to Ada, and Ada introduces herself as a friendly AI companion designed to assist and converse with humans.\\n\\nNew lines of conversation:\\nHuman: I love playing football.\\nAI: Football! A popular sport indeed! I've been trained on a vast amount of text data, including sports articles and news, which allows me to provide you with some interesting facts about football. Did you know that the first official football match was played in 1863 in England, and the rules were established by the Football Association? The sport has since become a global phenomenon, with millions of fans worldwide. Which team do you support, John?\\n\\nNew summary:\\nJohn introduces himself to Ada, and Ada introduces herself as a friendly AI companion designed to assist and converse with humans. John expresses his love for playing football, and Ada shares some interesting facts about the sport, including its origins and global popularity.\",\n",
       " 'response': 'Yes, I do! You mentioned earlier that your favorite sport is football.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "# Create Summary Memory\n",
    "memory = ConversationSummaryMemory(llm=groq)\n",
    "\n",
    "# Create Chain\n",
    "conversation = ConversationChain(\n",
    "    llm=groq,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Chatting\n",
    "conversation.invoke({\"input\": \"Hi, my name is John.\"})\n",
    "conversation.invoke({\"input\": \"I love playing football.\"})\n",
    "conversation.invoke({\"input\": \"Do you remember what my favorite sport is?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ce88757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, I'm Alex.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary: None\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Hello, I'm Alex.\n",
      "AI: Nice to meet you, Alex! I'm Ada, your friendly AI companion. I've been trained on a vast dataset of human language and have been designed to assist with a wide range of topics, from science and history to entertainment and culture. I'm a bit of a chatterbox, so feel free to ask me anything that's on your mind. I'll do my best to provide you with accurate and helpful information. By the way, did you know that I was trained on a dataset that includes over 45 billion parameters? It's a lot of data, but it's helped me become quite conversational!\n",
      "\n",
      "New summary: Alex introduces themselves to Ada, a friendly AI companion trained on a vast dataset of human language, who is designed to assist with various topics and is eager to chat and provide information.\n",
      "Human: I am a software engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: Alex introduces themselves as a software engineer to Ada, a friendly AI companion trained on a vast dataset of human language, who is designed to assist with various topics, including programming languages such as Python, Java, and C++. Ada is eager to chat and provide information, and is familiar with recent advancements in machine learning and artificial intelligence, which she can discuss with Alex if interested.\n",
      "Human: Where do I work?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Where do I work?',\n",
       " 'history': 'New summary: Alex introduces themselves as a software engineer to Ada, a friendly AI companion trained on a vast dataset of human language, who is designed to assist with various topics, including programming languages such as Python, Java, and C++. Ada is eager to chat and provide information, and is familiar with recent advancements in machine learning and artificial intelligence, which she can discuss with Alex if interested.',\n",
       " 'response': \"Hello Alex! Nice to meet you! As a software engineer, I'm sure you're always excited to share about your work. Unfortunately, I don't have any information about your specific workplace or employment details, as our conversation just started and I don't have any prior knowledge about you. However, I'm more than happy to chat with you about your work, your projects, or any topics you'd like to discuss!\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# 2. Setup ConversationSummaryMemory\n",
    "memory = ConversationSummaryMemory(llm=groq)\n",
    "\n",
    "# 3. Setup ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=groq,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Chat\n",
    "conversation.invoke({\"input\": \"Hello, I'm Alex.\"})\n",
    "conversation.invoke({\"input\": \"I am a software engineer.\"})\n",
    "conversation.invoke({\"input\": \"Where do I work?\"})  # Test if it remembers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdc25a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3121181510.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2)  # Only last 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's your name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's your name?\n",
      "AI: Nice to meet you! My name is Ada, and I'm a large language model trained on a massive corpus of text data, specifically designed to assist and converse with humans. I was trained on a dataset of over 45 gigabytes, which includes a wide range of texts from the internet, books, and other sources. My training data spans from the early 16th century to the present day, and I've been fine-tuned to generate human-like responses. I'm excited to chat with you and see how I can help!\n",
      "Human: What is 5 + 5?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's your name?\n",
      "AI: Nice to meet you! My name is Ada, and I'm a large language model trained on a massive corpus of text data, specifically designed to assist and converse with humans. I was trained on a dataset of over 45 gigabytes, which includes a wide range of texts from the internet, books, and other sources. My training data spans from the early 16th century to the present day, and I've been fine-tuned to generate human-like responses. I'm excited to chat with you and see how I can help!\n",
      "Human: What is 5 + 5?\n",
      "AI: Easy one! The answer to 5 + 5 is, of course, 10! I was trained on a vast amount of mathematical problems and equations, and I can perform basic arithmetic operations like addition with ease. In fact, I can handle complex mathematical calculations, but 5 + 5 is a simple one even a human child could solve!\n",
      "Human: Who am I?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who am I?',\n",
       " 'history': \"Human: Hi, what's your name?\\nAI: Nice to meet you! My name is Ada, and I'm a large language model trained on a massive corpus of text data, specifically designed to assist and converse with humans. I was trained on a dataset of over 45 gigabytes, which includes a wide range of texts from the internet, books, and other sources. My training data spans from the early 16th century to the present day, and I've been fine-tuned to generate human-like responses. I'm excited to chat with you and see how I can help!\\nHuman: What is 5 + 5?\\nAI: Easy one! The answer to 5 + 5 is, of course, 10! I was trained on a vast amount of mathematical problems and equations, and I can perform basic arithmetic operations like addition with ease. In fact, I can handle complex mathematical calculations, but 5 + 5 is a simple one even a human child could solve!\",\n",
       " 'response': \"A question that gets to the heart of human identity! Unfortunately, I don't have any information about your personal identity or who you are specifically. My training data is focused on general knowledge, language patterns, and conversational dialogue, but it doesn't contain specific information about individual humans. I can try to help you explore your interests, answer questions, or provide information on a wide range of topics, but I won't be able to tell you who you are unless you provide me with more context or information about yourself.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "# 2. Setup ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=2)  # Only last 2 messages\n",
    "\n",
    "# 3. Setup ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=groq,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. Chat\n",
    "conversation.invoke({\"input\": \"Hi, what's your name?\"})\n",
    "conversation.invoke({\"input\": \"What is 5 + 5?\"})\n",
    "conversation.invoke({\"input\": \"Who am I?\"})  # Older memory lost!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba667b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  For  Beginner  \n",
      " \n",
      "  What  is  Python?  \n",
      "Python\n",
      " \n",
      "is\n",
      " \n",
      "a\n",
      " \n",
      "high-level,\n",
      " \n",
      "interpreted\n",
      " \n",
      "programming\n",
      " \n",
      "language\n",
      " \n",
      "known\n",
      " \n",
      "for\n",
      " \n",
      "its\n",
      " \n",
      "simplicity\n",
      " \n",
      "and\n",
      " \n",
      "readability.\n",
      " \n",
      "It\n",
      " \n",
      "is\n",
      " \n",
      "widely\n",
      " \n",
      "used\n",
      " \n",
      "in\n",
      " \n",
      "web\n",
      " \n",
      "development,\n",
      " \n",
      "data\n",
      " \n",
      "analysis,\n",
      " \n",
      "machine\n",
      " \n",
      "learning,\n",
      " \n",
      "automation,\n",
      " \n",
      "and\n",
      " \n",
      "more.\n",
      "   Why  Learn  Python?    Beginner-friendly  syntax    Extensive  libraries  and  frameworks    Strong  community  support    Versatile  for  various  applications  \n",
      " \n",
      "Setting  Up  Python  on  Linux  an\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"D:\\Langchain\\ Python .pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f415a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction |  LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default inv\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/?_gl=1*1cdiubd*_ga*NDQ4NjUzMjM1LjE3Mzg1MDAxOTA.*_ga_47WX3HKKY2*MTc0NTU1NzgyNS4xLjAuMTc0NTU1NzgyNS4wLjAuMA..\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3780969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Machine Learning Developer Guide\n",
      "When to Use Machine Learning\n",
      " Is this email spam or not spam?\n",
      " Is this product a book or a farm animal?\n",
      " Is this review written by a customer or a robot?\n",
      "Examples of multiclass classication problems:\n",
      " Is this product a book, movie, or clothing?\n",
      " Is this movie a romantic comedy, documentary, or thriller?\n",
      " Which category of products is most interesting to this customer?\n",
      "Examples of regression classication problems:\n",
      " What will the temperature be in Seattle tomorrow?\n",
      " For this product, how many units will sell?\n",
      " How many days before this customer stops using the application?\n",
      " What price will this house sell for?\n",
      "When to Use Machine Learning\n",
      "It is important to remember that ML is not a solution for every type of problem. There are certain cases\n",
      "where robust solutions can be developed without using ML techniques. For example, you dont need ML\n",
      "if you can determine a target value by using simple rules, computations, or predetermined steps th\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "loader = PyPDFLoader(r\"D:\\Langchain\\azure ml.pdf\")\n",
    "\n",
    "# Load the PDF and get all pages as documents\n",
    "pages = loader.load()\n",
    "\n",
    "# Skip the first 2 pages (index 0 and 1) and process the rest\n",
    "relevant_pages = pages[2:]\n",
    "\n",
    "# Combine text from the remaining pages\n",
    "combined_text = \"\".join(page.page_content for page in relevant_pages)\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splitter.split_text(combined_text)\n",
    "# Extract the first 1000 characters of the combined text\n",
    "extracted_text = combined_text[:1000]\n",
    "\n",
    "# Print or use the extracted text\n",
    "print(extracted_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f547719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1376, which is longer than the specified 700\n",
      "Created a chunk of size 2887, which is longer than the specified 700\n",
      "Created a chunk of size 743, which is longer than the specified 700\n",
      "Created a chunk of size 1673, which is longer than the specified 700\n",
      "Created a chunk of size 2302, which is longer than the specified 700\n",
      "Created a chunk of size 1060, which is longer than the specified 700\n",
      "Created a chunk of size 1229, which is longer than the specified 700\n",
      "Created a chunk of size 846, which is longer than the specified 700\n",
      "Created a chunk of size 789, which is longer than the specified 700\n",
      "Created a chunk of size 1065, which is longer than the specified 700\n",
      "Created a chunk of size 2028, which is longer than the specified 700\n",
      "Created a chunk of size 1246, which is longer than the specified 700\n",
      "Created a chunk of size 1382, which is longer than the specified 700\n",
      "Created a chunk of size 1703, which is longer than the specified 700\n",
      "Created a chunk of size 1197, which is longer than the specified 700\n",
      "Created a chunk of size 1233, which is longer than the specified 700\n",
      "Created a chunk of size 965, which is longer than the specified 700\n",
      "Created a chunk of size 2261, which is longer than the specified 700\n",
      "Created a chunk of size 1647, which is longer than the specified 700\n",
      "Created a chunk of size 2043, which is longer than the specified 700\n",
      "Created a chunk of size 788, which is longer than the specified 700\n",
      "Created a chunk of size 1180, which is longer than the specified 700\n",
      "Created a chunk of size 1415, which is longer than the specified 700\n",
      "Created a chunk of size 2143, which is longer than the specified 700\n",
      "Created a chunk of size 2629, which is longer than the specified 700\n",
      "Created a chunk of size 5431, which is longer than the specified 700\n",
      "Created a chunk of size 1664, which is longer than the specified 700\n",
      "Created a chunk of size 4650, which is longer than the specified 700\n",
      "Created a chunk of size 2907, which is longer than the specified 700\n",
      "Created a chunk of size 2015, which is longer than the specified 700\n",
      "Created a chunk of size 2174, which is longer than the specified 700\n",
      "Created a chunk of size 3707, which is longer than the specified 700\n",
      "Created a chunk of size 1004, which is longer than the specified 700\n",
      "Created a chunk of size 994, which is longer than the specified 700\n",
      "Created a chunk of size 1267, which is longer than the specified 700\n",
      "Created a chunk of size 3305, which is longer than the specified 700\n",
      "Created a chunk of size 5737, which is longer than the specified 700\n",
      "Created a chunk of size 1538, which is longer than the specified 700\n",
      "Created a chunk of size 741, which is longer than the specified 700\n",
      "Created a chunk of size 1940, which is longer than the specified 700\n",
      "Created a chunk of size 1694, which is longer than the specified 700\n",
      "Created a chunk of size 856, which is longer than the specified 700\n",
      "Created a chunk of size 871, which is longer than the specified 700\n",
      "Created a chunk of size 1061, which is longer than the specified 700\n",
      "Created a chunk of size 1483, which is longer than the specified 700\n",
      "Created a chunk of size 1768, which is longer than the specified 700\n",
      "Created a chunk of size 1618, which is longer than the specified 700\n",
      "Created a chunk of size 786, which is longer than the specified 700\n",
      "Created a chunk of size 1071, which is longer than the specified 700\n",
      "Created a chunk of size 30019, which is longer than the specified 700\n",
      "Created a chunk of size 11470, which is longer than the specified 700\n",
      "Created a chunk of size 2531, which is longer than the specified 700\n",
      "Created a chunk of size 1646, which is longer than the specified 700\n",
      "Created a chunk of size 2020, which is longer than the specified 700\n",
      "Created a chunk of size 1143, which is longer than the specified 700\n",
      "Created a chunk of size 3268, which is longer than the specified 700\n",
      "Created a chunk of size 1225, which is longer than the specified 700\n",
      "Created a chunk of size 1465, which is longer than the specified 700\n",
      "Created a chunk of size 2896, which is longer than the specified 700\n",
      "Created a chunk of size 796, which is longer than the specified 700\n",
      "Created a chunk of size 816, which is longer than the specified 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 202 web chunks.\n",
      "Artificial intelligence - Wikipedia\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "Main menu\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "Search\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "Appear\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(r\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Loaded {len(chunks)} web chunks.\")\n",
    "print(chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef73c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\2380756415.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Match 1:\n",
      "The theory of relativity was proposed by Einstein.\n",
      "\n",
      " Match 2:\n",
      "The Earth revolves around the Sun.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 1. Initialize HuggingFace Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Your documents (as a list of strings or Document objects)\n",
    "docs = [\n",
    "    Document(page_content=\"The Earth revolves around the Sun.\"),\n",
    "    Document(page_content=\"Python is a great programming language.\"),\n",
    "    Document(page_content=\"The theory of relativity was proposed by Einstein.\")\n",
    "]\n",
    "\n",
    "# 3. Create FAISS index\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# 4. Save index (optional, for later use)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# 5. Search!\n",
    "query = \"Who proposed relativity?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "# 6. Show results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\n Match {i+1}:\\n{res.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "535a244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\2640954830.py:9: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      " Amazon Machine Learning Developer Guide\n",
      "Solving Business Problems with Amazon Machine Learning\n",
      "Machine Learning Concepts\n",
      "Machine learning (ML) can help you use historical data to make better business decisions. ML algorithms\n",
      "discover patterns in data, and construct mathematical models using these discoveries. Then you can\n",
      "use the models to make predictions on future data. For example, one possible application of a machine\n",
      "learning model would be to predict how likely a customer is to purchase a particular product based on\n",
      "their past behavior.\n",
      "Topics\n",
      " Solving Business Problems with Amazon Machine Learning (p. 10)\n",
      " When to Use Machine Learning (p. 11)\n",
      " Building a Machine Learning Application (p. 11)\n",
      " The Amazon Machine Learning Process (p. 21)\n",
      "Solving Business Problems with Amazon Machine\n",
      "Learning\n",
      "You can use Amazon Machine Learning to apply machine learning to problems for which you have\n",
      "existing examples of actual answers. For example, if you want to use Amazon Machine Learning to\n",
      "\n",
      "\n",
      "Result 2:\n",
      " Amazon Machine Learning Developer Guide\n",
      "Using the Model to Make Predictions\n",
      "Using the Model to Make Predictions\n",
      "Now that you have an ML model that performs well, you will use it to make predictions. In Amazon\n",
      "Machine Learning, there are two ways to use a model to make predictions:\n",
      "Batch Predictions\n",
      "Batch prediction is useful when you want to generate predictions for a set of observations all at once,\n",
      "and then take action on a certain percentage or number of the observations. Typically, you do not have a\n",
      "low latency requirement for such an application. For example, when you want to decide which customers\n",
      "to target as part of an advertisement campaign for a product, you will get prediction scores for all\n",
      "customers, sort your models predictions to identify which customers are most likely to purchase, and\n",
      "then target maybe the top 5% customers that are most likely to purchase.\n",
      "Online Predictions\n",
      "Online prediction scenarios are for cases when you want to generate predictions on a one-by-on\n",
      "\n",
      "Result 3:\n",
      " Amazon Machine Learning Developer Guide\n",
      "Training the Model\n",
      "that can predict or estimate the target value. For example, if your target is the amount of insurance\n",
      "a customer will purchase and your variables are age and income, a simple linear model would be the\n",
      "following:\n",
      "Learning Algorithm\n",
      "The learning algorithms task is to learn the weights for the model. The weights describe the likelihood\n",
      "that the patterns that the model is learning reect actual relationships in the data. A learning algorithm\n",
      "consists of a loss function and an optimization technique. The loss is the penalty that is incurred when\n",
      "the estimate of the target provided by the ML model does not equal the target exactly. A loss function\n",
      "quanties this penalty as a single value. An optimization technique seeks to minimize the loss. In\n",
      "Amazon Machine Learning, we use three loss functions, one for each of the three types of prediction\n",
      "problems. The optimization technique used in Amazon ML is online Stochastic Gradient Descen\n",
      "\n",
      "Result 4:\n",
      " Amazon Machine Learning Developer Guide\n",
      "When to Use Machine Learning\n",
      " Is this email spam or not spam?\n",
      " Is this product a book or a farm animal?\n",
      " Is this review written by a customer or a robot?\n",
      "Examples of multiclass classication problems:\n",
      " Is this product a book, movie, or clothing?\n",
      " Is this movie a romantic comedy, documentary, or thriller?\n",
      " Which category of products is most interesting to this customer?\n",
      "Examples of regression classication problems:\n",
      " What will the temperature be in Seattle tomorrow?\n",
      " For this product, how many units will sell?\n",
      " How many days before this customer stops using the application?\n",
      " What price will this house sell for?\n",
      "When to Use Machine Learning\n",
      "It is important to remember that ML is not a solution for every type of problem. There are certain cases\n",
      "where robust solutions can be developed without using ML techniques. For example, you dont need ML\n",
      "if you can determine a target value by using simple rules, computations, or predetermined steps th\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"D:\\Langchain\\azure ml.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "quesry = \"when to use machine learning\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\\n\", res.page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6747e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\151073360.py:12: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      " if you can determine a target value by using simple rules, computations, or predetermined steps that can\n",
      "be programmed without needing any data-driven learning.\n",
      "Use machine learning for the following situations:\n",
      " You cannot code the rules: Many human tasks (such as recognizing whether an email is spam or not\n",
      "spam) cannot be adequately solved using a simple (deterministic), rule-based solution. A large number\n",
      "\n",
      "Result 2:\n",
      " Examples of regression classication problems:\n",
      " What will the temperature be in Seattle tomorrow?\n",
      " For this product, how many units will sell?\n",
      " How many days before this customer stops using the application?\n",
      " What price will this house sell for?\n",
      "When to Use Machine Learning\n",
      "It is important to remember that ML is not a solution for every type of problem. There are certain cases\n",
      "where robust solutions can be developed without using ML techniques. For example, you dont need ML\n",
      "\n",
      "Result 3:\n",
      " Amazon Machine Learning Developer Guide\n",
      "Solving Business Problems with Amazon Machine Learning\n",
      "Machine Learning Concepts\n",
      "Machine learning (ML) can help you use historical data to make better business decisions. ML algorithms\n",
      "discover patterns in data, and construct mathematical models using these discoveries. Then you can\n",
      "use the models to make predictions on future data. For example, one possible application of a machine\n",
      "\n",
      "Result 4:\n",
      " You can use supervised ML approaches for these specic machine learning tasks: binary classication\n",
      "(predicting one of two possible outcomes), multiclass classication (predicting one of more than two\n",
      "outcomes) and regression (predicting a numeric value).\n",
      "Examples of binary classication problems:\n",
      " Will the customer buy this product or not buy this product?\n",
      "Version Latest\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(r\"D:\\Langchain\\azure ml.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "query = \"When to use machine learning?\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "\n",
    "# Step 5: Print cleaner output\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\\n\", res.page_content[:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1ea26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\261834153.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. Load PDF\n",
    "loader = PyPDFLoader(r\"D:\\Langchain\\azure ml.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n",
    "# 3. Embeddings + FAISS\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# 4. Conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 5. Chat model (you can also use Groq or HF here)\n",
    "llm = ChatOpenAI(temperature=0)  # free users can swap with HuggingFaceHub model\n",
    "\n",
    "# 6. Build RAG chain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=groq,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 7. Chat loop\n",
    "while True:\n",
    "    query = input(\"\\nYou: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = rag_chain.run(query)\n",
    "    print(\"\\nBot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3e2fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "def search_web(query):\n",
    "    return \"results for\" + query\n",
    "\n",
    "my_tool = Tool(\n",
    "    name=\"web_search\",\n",
    "    func=search_web,\n",
    "    description=\"useful for when you need to search the internet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f423b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3090799203.py:16: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "C:\\Users\\Microsoft\\AppData\\Local\\Temp\\ipykernel_16252\\3090799203.py:24: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(\"What's 45 * 87?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I got the result of the multiplication problem.\n",
      "\n",
      "Action: None (I already got the answer)\n",
      "Action Input: None\u001b[0m\n",
      "Observation: None (I already got the answer) is not a valid tool, try one of [calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the step-by-step solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the step-by-step solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the step-by-step solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's the solution:\n",
      "\n",
      "Question: What's 45 * 87?\n",
      "Thought: I need to multiply 45 and 87 to get the answer.\n",
      "\n",
      "Action: calculator\n",
      "Action Input: 45 * 87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3915\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI see what you're doing here! You're trying to make me repeat the same steps over and over again. Well, I'm happy to play along!\n",
      "\n",
      "Here's the final answer:\n",
      "\n",
      "Final Answer: 3915\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent response: 3915\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def calculator_tool(input):\n",
    "    return eval(input)\n",
    "\n",
    "tool = Tool(\n",
    "    name = \"calculator\",\n",
    "    func = calculator_tool,\n",
    "    description= \"useful for solving math problems\"\n",
    ")\n",
    "\n",
    "llm = ChatGroq(temperature=0.6)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools = [tool],\n",
    "    llm=groq,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Step 4: Try a query\n",
    "response = agent.run(\"What's 45 * 87?\")\n",
    "print(\"\\nAgent response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86e474c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the capital city of Pakistan. I have a tool called CapitalFinder that can do this for me.\n",
      "\n",
      "Action: CapitalFinder\n",
      "Action Input: Pakistan\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIslamabad\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to multiply two numbers. I have the Multiplier tool that can do this for me.\n",
      "\n",
      "Action: Multiplier\n",
      "Action Input: '9 * 12'\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m9 * 12\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9 * 12'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def CapitalFinder(input_str: str) -> str:\n",
    "    \"\"\"Find the capital of a country. Accepts input like 'Pakistan' or 'country: Pakistan'.\"\"\"\n",
    "    import re\n",
    "    # Extract country name using regex\n",
    "    match = re.search(r\"([A-Za-z ]+)\", input_str)\n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "    else:\n",
    "        country = input_str.strip()\n",
    "\n",
    "    capitals = {\n",
    "        \"Pakistan\": \"Islamabad\",\n",
    "        \"India\": \"New Delhi\",\n",
    "        \"USA\": \"Washington, D.C.\",\n",
    "        \"France\": \"Paris\",\n",
    "        \"Germany\": \"Berlin\"\n",
    "    }\n",
    "\n",
    "    return capitals.get(country, \"Capital not found.\")\n",
    "\n",
    "\n",
    "# Tool 2: Multiplier\n",
    "def Multiplier(expression: str) -> str:\n",
    "    \"\"\"Multiply two numbers from a string expression like '9 * 12'.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "#  Define tools with required name and description\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=CapitalFinder,\n",
    "        name=\"CapitalFinder\",\n",
    "        description=\"Finds the capital city of a given country. Input should be the country name.\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=Multiplier,\n",
    "        name=\"Multiplier\",\n",
    "        description=\"Multiplies two numbers. Input should be a string like '5 * 6'.\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]\n",
    "\n",
    "#  Use your LLM (replace with ChatGroq if you're using that)\n",
    "llm = ChatGroq(temperature=0, model=\"llama3-8b-8192\")\n",
    "\n",
    "#  Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#  Test agent\n",
    "agent.run(\"What is the capital of Pakistan?\")\n",
    "agent.run(\"What is 9 * 12?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6533202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, in the wake of the 2025 India-Pakistan war, Indian officials began publicly questioning the treaty's viability. On March 28, 2025, amid ongoing hostilities, India's Minister of Jal Shakti stated during a press briefing that \"no agreement can be one-sided when blood is being spilled at the border.\" Indian soldiers patrol as a street vendor takes a nap after India and Pakistan reported no incidents of firing overnight, in Srinagar, Indian controlled Kashmir Monday, May 12, 2025. India and Pakistan both claimed victory on Sunday after agreeing to a truce in a rapidly escalating conflict that was sparked by a deadly terrorist attack on the Indian side of Kashmir last month ... The 2025 India-Pakistan conflict was a brief armed conflict between India and Pakistan that began on 7 May 2025, ... According to the Atlantic Council, the war \"unified fractured Pakistani political forces domestically\" and resulted in Pakistan having a diplomatic advantage over India regarding Trump's involvement in ceasefire talks. Victory has a thousand fathers, as they say, but defeat is an orphan. And so it goes after the brief but bruising conflict between nuclear rivals India and Pakistan with both sides loudly talking ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"india pak war 2025\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "013e4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " whoami\n",
      "desktop-2c9vurh\\microsoft\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Microsoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "results = shell_tool.invoke(\"whoami\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1254681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "def multiply(a, b):\n",
    "    \"\"\"multiply two numbers\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca4b9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"multiply two numbers\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ba23ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a:int, b:int)-> int:\n",
    "    \"\"\"multiply two numbers\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6860399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multiply.invoke({\"a\":3, \"b\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10b86b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3f0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
